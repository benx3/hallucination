# ğŸ§  Hallucination Detection Research Project

A comprehensive research framework for comparing **direct prompting** vs **self-critique prompting** for hallucination detection in Vietnamese Q&A across 4 LLM APIs: **OpenAI GPT-4**, **DeepSeek**, **Google Gemini Pro**, and **Ollama** (local models).

## âœ¨ Key Features

### ğŸ¯ Enhanced UI Analytics
- **Visual Case Indicators**: ğŸ¯ Direct prompting vs ğŸ§  Self-critique prompting badges
- **Step-by-Step Analysis**: Parse and display self-critique reasoning process (BÆ°á»›c 1â†’2â†’3)
- **Prompt Transparency**: View exact prompts used for each case
- **Interactive Filtering**: Filter hallucination cases by API, dataset, and prompt strategy

### ğŸ“Š Comprehensive Model Comparison
- **Automated Ranking System**: Composite scoring across multiple metrics
- **Performance Analysis**: Correctness, uncertainty detection, hallucination rates
- **Cross-Dataset Evaluation**: Performance consistency across different domains
- **Real-time Analytics**: Live dashboard with 716+ hallucination cases analyzed

### ğŸš€ Multi-API Architecture
- **Unified Interface**: Single codebase handles all 4 LLM providers
- **Consistent Output**: Standardized CSV schema across all APIs
- **Error Resilience**: Robust handling of API timeouts and rate limits
- **Environment Configuration**: Easy switching between models and datasets

## ğŸš€ Quick Start

### Option 1: Interactive Menu
```bash
python main.py
```

### Option 2: Enhanced Web Dashboard
```bash
# Setup configuration
copy configs/config.example.json configs/config.json
# Edit configs/config.json with your API keys

# Launch enhanced UI (recommended)
launch_ui.bat
# OR manually:
streamlit run ui/app.py --server.port 8502
```

### Option 3: Complete Experiments
```bash
# Run all APIs and datasets
python run_comprehensive_experiments.py

# Analyze model performance
python analyze_models.py
```

## ğŸ“ Enhanced Project Structure

```
ğŸ“¦ halu2/
â”œâ”€â”€ ğŸ“‚ src/                          # Core logic
â”‚   â”œâ”€â”€ api_runner.py                # Unified API interface with prompt saving
â”‚   â””â”€â”€ evaluator.py                 # Advanced grading and metrics
â”œâ”€â”€ ğŸ“‚ ui/                           # Enhanced Streamlit dashboard  
â”‚   â”œâ”€â”€ app.py                       # Main UI with hallucination analysis
â”‚   â”œâ”€â”€ experiment_runner.py         # Backend experiment management
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ analytics.py             # Basic analytics
â”‚       â””â”€â”€ enhanced_analytics.py    # Advanced hallucination visualization
â”œâ”€â”€ ğŸ“‚ data/                         # Datasets and organized results
â”‚   â”œâ”€â”€ astronomy_hard.csv           # Astronomy questions
â”‚   â”œâ”€â”€ mathematics_hard.csv         # Mathematics problems  
â”‚   â”œâ”€â”€ questions_50_hard.csv        # General knowledge
â”‚   â”œâ”€â”€ scientific_facts_basic.csv   # Science facts
â”‚   â””â”€â”€ results/                     # Results by API provider
â”‚       â”œâ”€â”€ openai/                  # GPT-4 results
â”‚       â”œâ”€â”€ deepseek/                # DeepSeek results  
â”‚       â”œâ”€â”€ gemini/                  # Gemini Pro results
â”‚       â””â”€â”€ ollama/                  # Local model results
â”œâ”€â”€ ğŸ“‚ configs/                      # API configuration
â”‚   â”œâ”€â”€ config_manager.py            # API key management
â”‚   â”œâ”€â”€ config.example.json          # Configuration template
â”‚   â””â”€â”€ config.json                  # Your API keys
â”œâ”€â”€ ğŸ“‚ scripts/                      # Analysis utilities
â”‚   â”œâ”€â”€ cross_model_comparison.py    # Cross-model analysis
â”‚   â”œâ”€â”€ analyze_patterns.py          # Pattern detection
â”‚   â””â”€â”€ prep_additional_datasets.py  # Data preparation
â”œâ”€â”€ ğŸ“‚ docs/                         # Documentation
â”‚   â”œâ”€â”€ QUICK_START.md               # Vietnamese quick start
â”‚   â”œâ”€â”€ UI_GUIDE.md                  # Dashboard guide
â”‚   â””â”€â”€ OLLAMA_SETUP.md              # Local model setup
â”œâ”€â”€ analyze_models.py                # Comprehensive model ranking
â”œâ”€â”€ run_comprehensive_experiments.py # Full experiment pipeline
â””â”€â”€ requirements.txt                 # Updated dependencies
```

## ğŸ§  Self-Critique Analysis

The project implements a sophisticated 3-step self-critique process:

1. **BÆ°á»›c 1 - NhÃ¡p**: Initial draft response
2. **BÆ°á»›c 2 - Tá»± kiá»ƒm**: Self-verification and critique  
3. **BÆ°á»›c 3 - Cuá»‘i cÃ¹ng**: Final refined answer

The enhanced UI automatically parses these steps and displays them with structured formatting, making it easy to understand the model's reasoning process.

## ğŸ† Current Model Rankings

Based on comprehensive analysis across all datasets:

1. **ğŸ¥‡ Google Gemini Pro** (56.2/100)
   - Best overall performance
   - Excellent uncertainty detection
   - Consistent across domains

2. **ğŸ¥ˆ OpenAI GPT-4** (49.7/100)
   - Strong consistency
   - Good balance of metrics
   - Reliable performance

3. **ğŸ¥‰ DeepSeek** (49.6/100)
   - Good value proposition
   - Competitive performance
   - Cost-effective option

4. **ğŸ Ollama (Local)** (35.2/100)
   - Privacy-focused option
   - No API costs
   - Suitable for sensitive data

## ğŸ“Š Advanced Features

### Hallucination Detection Logic
- **Correctness**: Vietnamese text normalization and containment checking
- **Uncertainty Detection**: Bilingual regex patterns for uncertainty expressions
- **Hallucination**: Confident but incorrect responses (NOT correct AND NOT uncertain)
- **Risk Scoring**: Question difficulty based on cross-model hallucination rates

### Enhanced UI Capabilities
- **Real-time Filtering**: Filter 716+ hallucination cases by multiple criteria
- **Visual Indicators**: Instant recognition of Direct vs Self-Critique cases
- **Step Parsing**: Automatic extraction and formatting of reasoning steps
- **Metrics Dashboard**: Live performance comparison across all APIs

### Data Flow Architecture
```
Input datasets â†’ Multi-API runners â†’ Raw results â†’ Advanced grading â†’ Enhanced analytics â†’ Interactive dashboard
```

## ğŸ”§ Configuration

### API Setup
Create `configs/config.json` from the example:
```json
{
  "openai": {
    "api_key": "your-openai-key"
  },
  "deepseek": {
    "api_key": "your-deepseek-key",
    "base_url": "https://api.deepseek.com"
  },
  "gemini": {
    "api_key": "your-gemini-key"
  },
  "ollama": {
    "base_url": "http://localhost:11434"
  }
}
```

### Environment Variables
```bash
MODEL_NAME=llama3.2              # For Ollama
API_PROVIDER=openai              # openai, deepseek, gemini, ollama  
INPUT_CSV=questions_50_hard.csv  # Input dataset
TIMEOUT_S=120                    # API timeout
```

## ğŸš€ Usage Examples

### Running Specific Analysis
```bash
# Single API experiment
python src/api_runner.py --api openai --dataset mathematics_hard.csv

# Model comparison with detailed breakdown
python analyze_models.py

# Launch enhanced dashboard
streamlit run ui/app.py --server.port 8502
```

### Analyzing Hallucination Cases
1. Launch the web dashboard: `launch_ui.bat`
2. Navigate to "Hallucination Cases Analysis" 
3. Filter by API, dataset, or prompt strategy
4. View step-by-step self-critique reasoning
5. Analyze visual indicators and patterns

## ğŸ“š Documentation

- [`docs/QUICK_START.md`](docs/QUICK_START.md) - Vietnamese quick start guide
- [`docs/UI_GUIDE.md`](docs/UI_GUIDE.md) - Enhanced dashboard usage
- [`docs/OLLAMA_SETUP.md`](docs/OLLAMA_SETUP.md) - Local model configuration
- [`docs/README_COMPLETE_EXPERIMENT.md`](docs/README_COMPLETE_EXPERIMENT.md) - Full workflow

## ğŸ› ï¸ Installation

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd halu2
   pip install -r requirements.txt
   ```

2. **Configure APIs**
   ```bash
   copy configs/config.example.json configs/config.json
   # Edit config.json with your API keys
   ```

3. **Launch Dashboard**
   ```bash
   launch_ui.bat
   ```

## ğŸ§ª Advanced Development

### Vietnamese Text Handling
- Bilingual prompt support (Vietnamese/English)
- Custom normalization for Vietnamese proper nouns
- Uncertainty pattern detection in both languages

### API Integration Patterns
- **OpenAI**: Official client with structured outputs
- **DeepSeek**: OpenAI-compatible client
- **Gemini**: Google AI client with safety settings
- **Ollama**: Direct HTTP requests to local server

### Output Formats
- **CSV**: Raw responses and graded results with prompt data
- **JSON**: Metrics and configuration files
- **Word**: Academic reports with charts and analysis
- **Interactive**: Real-time Streamlit dashboard

## ğŸ¤ Contributing

1. Follow existing patterns in `src/api_runner.py`
2. Maintain consistent CSV output schema
3. Update evaluation logic and uncertainty patterns together
4. Test with multiple APIs before submitting changes

## ğŸ“„ License

This research project is for academic use. Please cite appropriately if used in publications.

---

For detailed usage instructions, see the Vietnamese guide: [`docs/QUICK_START.md`](docs/QUICK_START.md)