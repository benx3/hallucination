# ğŸ§  Hallucination Detection Research Project

A comprehensive research framework for comparing direct prompting vs self-critique prompting for hallucination detection in Vietnamese Q&A across multiple LLM APIs.

## ğŸš€ Quick Start

### Option 1: Interactive Menu
```bash
python main.py
```

### Option 2: Web UI
```bash
# Setup config first
copy configs/config.example.json configs/config.json
# Edit configs/config.json with your API keys

# Launch UI
python -m streamlit run ui/app.py
# OR double-click: launch_ui.bat
```

### Option 3: Direct Experiment
```bash
python run_experiment.py
```

## ğŸ“ Project Structure

```
ğŸ“¦ halu2/
â”œâ”€â”€ ğŸ“‚ src/                    # Core source code
â”‚   â”œâ”€â”€ api_runner.py          # Unified API runner (OpenAI, DeepSeek, Gemini, Ollama)
â”‚   â””â”€â”€ evaluator.py           # Evaluation and report generation
â”œâ”€â”€ ğŸ“‚ ui/                     # Web interface
â”‚   â”œâ”€â”€ app.py                 # Main Streamlit application
â”‚   â”œâ”€â”€ experiment_runner.py   # UI backend logic
â”‚   â””â”€â”€ components/            # UI components
â”œâ”€â”€ ğŸ“‚ configs/                # Configuration management
â”‚   â”œâ”€â”€ config_manager.py      # API key management
â”‚   â”œâ”€â”€ config.example.json    # Configuration template
â”‚   â””â”€â”€ config.json            # Your API keys (create from example)
â”œâ”€â”€ ğŸ“‚ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ prep_*.py             # Data preparation
â”‚   â”œâ”€â”€ analyze_patterns.py   # Pattern analysis
â”‚   â””â”€â”€ cross_model_comparison.py
â”œâ”€â”€ ğŸ“‚ docs/                   # Documentation
â”‚   â”œâ”€â”€ QUICK_START.md         # Quick start guide
â”‚   â”œâ”€â”€ README_UI.md           # UI documentation
â”‚   â””â”€â”€ README_COMPLETE_EXPERIMENT.md
â”œâ”€â”€ ğŸ“‚ data/                   # Datasets and results
â”‚   â”œâ”€â”€ TruthfulQA.csv             # TruthfulQA research dataset (817 questions)
â”‚   â”œâ”€â”€ scientific_facts_basic.csv # Scientific facts dataset (100 questions)
â”‚   â””â”€â”€ results/               # Experiment outputs
â”œâ”€â”€ ğŸ“„ main.py                 # Main entry point
â”œâ”€â”€ ğŸ“„ run_experiment.py       # Complete experiment runner
â””â”€â”€ ğŸ“„ launch_ui.bat          # Windows UI launcher
```

## ğŸ¯ Features

### Multi-API Support
- âœ… **OpenAI** (GPT-3.5, GPT-4)
- âœ… **DeepSeek** (DeepSeek Chat, DeepSeek Coder)  
- âœ… **Google Gemini** (Gemini Pro, Gemini 1.5 Pro)
- âœ… **Ollama** (Local models: Llama, Qwen, etc.)

### Dual Prompting Strategy
- ğŸ¯ **Direct Prompting**: Simple factual assistant
- ğŸ”„ **Self-Critique Prompting**: 3-step process (draft â†’ self-check â†’ final answer)

### Comprehensive Evaluation
- ğŸ“Š **Correctness Detection**: Multi-format answer matching
- ğŸ¤” **Uncertainty Detection**: Vietnamese/English uncertainty patterns
- ğŸš¨ **Hallucination Detection**: Confident but incorrect responses
- ğŸ“ˆ **Cross-Model Comparison**: Performance across all APIs

### Rich Output Formats
- ğŸ“„ **Word Reports**: Academic-style experiment reports
- ğŸ“Š **JSON Metrics**: Structured performance data
- ğŸ¨ **Interactive Charts**: Plotly visualizations
- ğŸ“‹ **CSV Data**: Raw and processed results

## âš™ï¸ Configuration

### 1. API Keys Setup
```bash
# Copy template
cp configs/config.example.json configs/config.json

# Edit with your keys
{
  "apis": {
    "openai": {
      "api_key": "sk-your-openai-key",
      "models": ["gpt-3.5-turbo", "gpt-4"]
    },
    "deepseek": {
      "api_key": "sk-your-deepseek-key",
      "models": ["deepseek-chat"]
    },
    "gemini": {
      "api_key": "your-google-api-key",
      "models": ["gemini-pro"]
    },
    "ollama": {
      "base_url": "http://localhost:11434",
      "models": ["llama3.2", "qwen2.5"]
    }
  }
}
```

### 2. Environment Variables (Alternative)
```bash
export OPENAI_API_KEY="sk-..."
export DEEPSEEK_API_KEY="sk-..."
export GOOGLE_API_KEY="..."
```

## ğŸ“Š Datasets

### Included Datasets
- **TruthfulQA.csv** - Complete TruthfulQA research dataset with 817 questions
- **scientific_facts_basic.csv** - Curated scientific facts covering physics, chemistry, biology
- **natural_questions_50.csv** - Natural Questions dataset  
- **fever_claims_50.csv** - FEVER fact-checking claims

### Custom Datasets
Add CSV files to `data/` with columns:
- `question` - The question text
- `answer` - Expected correct answer(s)
- `category` - Optional question category

## ğŸ”¬ Research Workflow

### 1. Experiment Execution
```
Dataset â†’ API Runner â†’ Raw Responses â†’ Evaluator â†’ Graded Results + Reports
```

### 2. Metrics Calculated
- **Correctness Rate**: % of accurate responses
- **Hallucination Rate**: % of confident but incorrect responses  
- **Uncertainty Rate**: % of responses expressing uncertainty
- **Response Time**: Average generation latency
- **Token Usage**: Input/output token consumption

### 3. Analysis Types
- **Direct vs Self-Critique**: Prompting strategy comparison
- **Cross-API Performance**: Model capability analysis
- **Question Difficulty**: Pattern-based difficulty scoring
- **Hallucination Patterns**: Error type categorization

## ğŸ› ï¸ Development

### Requirements
```bash
pip install -r requirements.txt
```

### Key Dependencies
- `streamlit` - Web UI framework
- `openai` - OpenAI API client
- `google-generativeai` - Gemini API client
- `pandas` - Data processing
- `plotly` - Interactive visualizations
- `python-docx` - Word report generation

### Architecture
- **Unified API Runner**: Single interface for all LLM providers
- **Modular Evaluation**: Pluggable metrics and grading logic
- **Component-Based UI**: Reusable Streamlit components
- **Config-Driven**: JSON-based configuration management

## ğŸ“ˆ Results

### Output Structure
```
data/results/{api}/{dataset}/
â”œâ”€â”€ results_raw.csv          # Raw API responses
â”œâ”€â”€ results_graded.csv       # Evaluated responses
â”œâ”€â”€ metrics.json             # Performance metrics
â”œâ”€â”€ experiment_report.docx   # Word report
â””â”€â”€ pattern_analysis.txt     # Hallucination patterns
```

### Metrics Interpretation
- **High Correctness** + **Low Hallucination** = Reliable model
- **High Uncertainty** = Calibrated confidence
- **Improvement Delta** = Self-critique effectiveness

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new functionality
4. Update documentation
5. Submit pull request

## ğŸ“ Citation

```bibtex
@misc{hallucination_detection_2024,
  title={Hallucination Detection in Vietnamese Q&A: Direct vs Self-Critique Prompting},
  author={Your Name},
  year={2024},
  howpublished={GitHub Repository},
  url={https://github.com/your-repo/hallucination-detection}
}
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

ğŸ”— **Links**: [Quick Start](docs/QUICK_START.md) | [UI Guide](docs/README_UI.md) | [Technical Docs](.github/copilot-instructions.md)