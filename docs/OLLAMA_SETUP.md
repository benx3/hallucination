# Ollama Setup vÃ  Troubleshooting Guide

## ğŸ” Kiá»ƒm tra Ollama Ä‘ang cháº¡y

### Method 1: PowerShell (Windows)
```powershell
# Check process
Get-Process ollama -ErrorAction SilentlyContinue

# Check API 
Invoke-RestMethod -Uri "http://localhost:11434/api/tags" -Method Get

# List models
Invoke-RestMethod -Uri "http://localhost:11434/api/tags" | Select-Object -ExpandProperty models | Select-Object name
```

### Method 2: Command Line
```bash
# Check if server responds
curl http://localhost:11434/api/tags

# Or vá»›i Python
python -c "import requests; print(requests.get('http://localhost:11434/api/tags').json())"
```

## ğŸš€ Khá»Ÿi Ä‘á»™ng Ollama Server

### Windows:
```cmd
# Method 1: Command Prompt
ollama serve

# Method 2: Náº¿u khÃ´ng cÃ³ trong PATH
C:\Users\%USERNAME%\AppData\Local\Programs\Ollama\ollama.exe serve

# Method 3: PowerShell background
Start-Process ollama -ArgumentList "serve" -WindowStyle Hidden
```

### Linux/Mac:
```bash
# Foreground
ollama serve

# Background  
nohup ollama serve > /dev/null 2>&1 &
```

## ğŸ“š Quáº£n lÃ½ Models

### Pull models (download):
```bash
# Llama 3.2 (2GB)
ollama pull llama3.2

# Llama 3.2 with vision
ollama pull llama3.2-vision

# Other popular models
ollama pull llama3.1:8b
ollama pull codellama
ollama pull mistral
```

### List available models:
```bash
ollama list
```

### Remove models:
```bash
ollama rm llama3.2
```

## ğŸ”§ Troubleshooting

### Server khÃ´ng start:
1. **Check port 11434** cÃ³ bá»‹ chiáº¿m khÃ´ng:
   ```powershell
   netstat -ano | findstr 11434
   ```

2. **Kill existing process**:
   ```powershell
   Get-Process ollama | Stop-Process -Force
   ```

3. **Restart**:
   ```bash
   ollama serve
   ```

### API khÃ´ng respond:
1. **Firewall**: Äáº£m báº£o port 11434 khÃ´ng bá»‹ block
2. **Antivirus**: Whitelist ollama.exe
3. **Restart**: Restart ollama service

### Model khÃ´ng táº£i Ä‘Æ°á»£c:
1. **Check internet connection**
2. **Check disk space** (models can be large)
3. **Try different model**:
   ```bash
   ollama pull llama3.2:1b  # Smaller version
   ```

## âš™ï¸ Configuration

### Custom host/port:
```bash
# Set environment variables
export OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

### Windows Environment Variables:
```cmd
set OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

## ğŸ§ª Test Connection

### Python test:
```python
import requests

try:
    response = requests.get('http://localhost:11434/api/tags')
    if response.status_code == 200:
        print("âœ… Ollama is running")
        models = response.json()['models']
        print(f"ğŸ“š Available models: {len(models)}")
        for model in models:
            print(f"  - {model['name']}")
    else:
        print("âŒ Ollama API error")
except Exception as e:
    print(f"âŒ Connection failed: {e}")
```

### Quick API test:
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3.2", "prompt": "Hello", "stream": false}'
```

## ğŸ“– Auto-start Script

Sá»­ dá»¥ng script: `python scripts/check_ollama.py`

Script nÃ y sáº½:
- âœ… Check náº¿u Ollama Ä‘ang cháº¡y
- ğŸš€ Start server náº¿u chÆ°a cháº¡y  
- ğŸ“š List available models
- ğŸ§ª Test API connection

## ğŸ”— Useful Links

- **Ollama Official**: https://ollama.ai
- **Model Library**: https://ollama.ai/library
- **API Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Troubleshooting**: https://github.com/ollama/ollama/issues