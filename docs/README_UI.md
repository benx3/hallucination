# ğŸ§  Hallucination Detection Research - UI Version

## ğŸ¯ Tá»•ng quan

Giao diá»‡n web tÆ°Æ¡ng tÃ¡c cho nghiÃªn cá»©u hallucination detection vá»›i kháº£ nÄƒng:

- âœ… **Multi-API Support**: OpenAI, DeepSeek, Gemini Pro, Ollama
- âœ… **Multi-Dataset**: Chá»n datasets tá»« folder `data/`
- âœ… **Real-time Tracking**: Progress bars vÃ  status updates
- âœ… **Interactive Charts**: Plotly visualizations
- âœ… **Export Reports**: CSV, JSON, Text formats
- âœ… **Smart Caching**: KhÃ´ng re-run experiments Ä‘Ã£ cÃ³

## ğŸš€ Quick Start

### 1. Khá»Ÿi Ä‘á»™ng Demo (khÃ´ng cáº§n API keys)
```bash
# Xem demo vá»›i fake data
streamlit run demo_ui.py
```

### 2. Setup Environment cho Production
```bash
# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Setup API keys
set OPENAI_API_KEY=your_openai_key
set DEEPSEEK_API_KEY=your_deepseek_key  
set GOOGLE_API_KEY=your_google_key

# Start Ollama (optional)
ollama serve
ollama run llama3.2
```

### 3. Launch UI
```bash
# Tá»± Ä‘á»™ng (Windows)
.\launch_ui.bat

# Manual
streamlit run app.py
```

Truy cáº­p: **http://localhost:8501**

## ğŸ“ Folder Structure

```
hallucination-research/
â”œâ”€â”€ ğŸ¨ UI Files
â”‚   â”œâ”€â”€ app.py                     # Main Streamlit application
â”‚   â”œâ”€â”€ demo_ui.py                 # Demo vá»›i fake data
â”‚   â”œâ”€â”€ ui_experiment_runner.py    # Backend experiment logic
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ analytics.py           # Advanced analytics
â”‚
â”œâ”€â”€ ğŸ“Š Data & Results
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ TruthfulQA.csv          # Complete research dataset (817 questions)
â”‚       â”œâ”€â”€ scientific_facts_basic.csv # Scientific facts (100 questions)
â”‚       â””â”€â”€ results/              # All experiment results
â”‚           â”œâ”€â”€ openai/
â”‚           â”œâ”€â”€ deepseek/
â”‚           â”œâ”€â”€ gemini/
â”‚           â””â”€â”€ ollama/
â”‚
â”œâ”€â”€ ğŸ”§ Scripts (Updated for UI)
â”‚   â”œâ”€â”€ openai_run.py             # API inference scripts
â”‚   â”œâ”€â”€ deepseek_run.py
â”‚   â”œâ”€â”€ gemini_run.py
â”‚   â”œâ”€â”€ run_ollama_eval.py
â”‚   â”œâ”€â”€ grade_and_report.py       # Evaluation logic
â”‚   â””â”€â”€ cross_model_comparison.py # Analysis scripts
â”‚
â””â”€â”€ ğŸš€ Setup Files
    â”œâ”€â”€ requirements.txt           # Dependencies
    â”œâ”€â”€ launch_ui.bat             # Windows launcher
    â”œâ”€â”€ launch_ui.ps1             # PowerShell launcher
    â””â”€â”€ UI_GUIDE.md               # Detailed guide
```

## ğŸ® UI Features Overview

### ğŸ”§ Sidebar Configuration
- **API Selection**: Multi-select vá»›i availability checking
- **Model Selection**: Dropdown per API  
- **Dataset Selection**: Multi-select tá»« `data/` folder
- **One-click Launch**: Start all experiments

### ğŸ“Š Main Dashboard

#### Tab 1: ğŸ“ˆ Metrics Overview
- **Interactive Charts**: Accuracy vs Hallucination rates
- **Summary Stats**: Overall performance metrics
- **Real-time Updates**: Charts update as experiments complete

#### Tab 2: ğŸ“‹ Detailed Results  
- **Results Table**: Chi tiáº¿t má»—i experiment
- **Question Analysis**: Drill-down vÃ o individual questions
- **Error Tracking**: Failed experiments vá»›i error messages

#### Tab 3: ğŸ“„ Export Reports
- **Cross-Model Report**: Comprehensive comparison
- **CSV Export**: Raw data cho further analysis
- **Timestamped Downloads**: Automatic filename generation

## ğŸ” Advanced Analytics

### Real-time Features:
- â±ï¸ **Progress Tracking**: Overall + individual experiment progress
- ğŸ”„ **Auto-refresh**: UI updates without manual refresh
- ğŸ’¾ **Smart Caching**: Load existing results automatically
- ğŸš¨ **Error Handling**: Continue on failures, show detailed errors

### Charts & Visualizations:
- ğŸ“Š **Bar Charts**: Accuracy comparison across APIs
- ğŸ“ˆ **Line Charts**: Improvement trends
- ğŸ¯ **Scatter Plots**: Accuracy vs Hallucination trade-offs
- ğŸ•¸ï¸ **Radar Charts**: Multi-dimensional API performance

### Export Options:
- ğŸ“ **Text Reports**: Human-readable summaries
- ğŸ“Š **CSV Data**: Machine-readable results
- ğŸ“‹ **JSON Metrics**: Structured experiment metadata
- ğŸ• **Timestamped**: Automatic versioning

## âš™ï¸ Configuration

### Environment Variables
```bash
# Required for respective APIs
OPENAI_API_KEY=sk-...
DEEPSEEK_API_KEY=sk-...  
GOOGLE_API_KEY=AIza...

# Optional model overrides
OPENAI_MODEL=gpt-4o-mini
DEEPSEEK_MODEL=deepseek-chat
GEMINI_MODEL=gemini-1.5-flash
MODEL_NAME=llama3.2              # Ollama

# Optional path overrides  
DATA_DIR=data
RESULTS_DIR=data/results
TIMEOUT_S=300                    # Per-experiment timeout
```

### Streamlit Config (.streamlit/config.toml)
```toml
[server]
port = 8501
headless = false

[theme]
primaryColor = "#FF6B6B"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#262730"
```

## ğŸ”§ Development & Customization

### Adding New APIs:
1. Update `API_CONFIGS` in `app.py`
2. Create `{api_name}_run.py` script
3. Update `ui_experiment_runner.py`

### Adding New Metrics:
1. Modify `grade_and_report.py`
2. Update chart functions in `components/analytics.py`
3. Update export functions

### Custom Themes:
1. Modify `.streamlit/config.toml`
2. Add custom CSS vá»›i `st.markdown` trong `app.py`

## ğŸ“‹ Workflow Example

1. **Setup**: Set API keys, prepare datasets
2. **Configure**: Select APIs vÃ  datasets trong sidebar
3. **Launch**: Click "Start Experiments"
4. **Monitor**: Watch real-time progress vÃ  results
5. **Analyze**: Explore charts vÃ  detailed results
6. **Export**: Download reports cho paper/presentation

## ğŸ› Troubleshooting

### Common Issues:

**"No datasets found"**
```bash
python prep_additional_datasets.py
```

**"API unavailable"**
```bash
# Check environment
echo $OPENAI_API_KEY

# Set in current session
set OPENAI_API_KEY=your_key
```

**"Ollama connection failed"**
```bash
ollama serve
curl http://localhost:11434/api/tags
```

**"ModuleNotFoundError"**
```bash
pip install -r requirements.txt
```

### Debug Tips:
- Check console output trong terminal
- Inspect `data/results/` folder cho partial results
- Run individual scripts manually Ä‘á»ƒ isolate issues
- Use demo mode Ä‘á»ƒ test UI features

## ğŸ¯ Research Workflow

### For Academic Papers:
1. **Hypothesis Formation**: Use UI Ä‘á»ƒ explore initial results
2. **Experiment Design**: Configure systematic API/dataset combinations  
3. **Data Collection**: Run comprehensive experiments
4. **Analysis**: Use interactive charts Ä‘á»ƒ identify patterns
5. **Reporting**: Export data cho statistical analysis
6. **Visualization**: Screenshots cá»§a charts cho papers

### Best Practices:
- Start vá»›i demo Ä‘á»ƒ familiarize
- Test small experiments trÆ°á»›c large-scale runs
- Save intermediate results frequently
- Document experimental settings
- Version control configurations

## ğŸ† Benefits over Command Line

- âœ… **User-Friendly**: No need to remember commands
- âœ… **Visual Feedback**: Real-time progress vÃ  charts
- âœ… **Error Resilience**: Continue on individual failures
- âœ… **Data Exploration**: Interactive analysis tools
- âœ… **Export Ready**: One-click report generation
- âœ… **Reproducible**: Automatic configuration tracking

---

## ğŸš€ Ready to Start?

```bash
# Demo first (no setup required)
streamlit run demo_ui.py

# Then real experiments
streamlit run app.py
```

**Happy Researching! ğŸ§ ğŸ“Š**